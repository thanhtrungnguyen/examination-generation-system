{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fine-Tuning DeepSeek-R1-0528",
   "id": "1c6fea978d73a465"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Setting Up",
   "id": "461c218f87430b57"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import required libraries for authentication and environment variable management\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get token from environment variable\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "# Log in to Hugging Face Hub using the token from the environment\n",
    "if hf_token:\n",
    "    login(hf_token)\n",
    "    print(\"✅ Hugging Face login successful.\")\n",
    "else:\n",
    "    print(\"❌ HF_TOKEN not found. Please check your .env file.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Loading the Model and Tokenizer",
   "id": "29e4ec97c635cf8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import model and tokenizer classes, and torch for tensor operations\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n"
   ],
   "id": "5a4519f0b35dfb72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configure 4-bit quantization for efficient model loading\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ],
   "id": "172a6e39abf80d74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load tokenizer and model from Hugging Face Hub with quantization settings\n",
    "model_dir = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ],
   "id": "e188e94674721544",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check GPU availability and status\n",
    "!nvidia-smi"
   ],
   "id": "3e3c2417398de233",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Loading and Processing the Dataset",
   "id": "6afd3c259e2cc4c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the prompt template for training\n",
    "train_prompt_style = \"\"\"\n",
    "Please answer with one of the options in the bracket. Write reasoning in between <analysis></analysis>. Write the answer in between <answer></answer>.\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ],
   "id": "1ba95693b1afb563",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the end-of-sequence token from the tokenizer\n",
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "\n",
    "# Define a function to format prompts for the model\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for question, response in zip(inputs, outputs):\n",
    "        # Remove the \"Q:\" prefix from the question\n",
    "        question = question.replace(\"Q:\", \"\")\n",
    "\n",
    "        # Append the EOS token to the response if it's not already there\n",
    "        if not response.endswith(tokenizer.eos_token):\n",
    "            response += tokenizer.eos_token\n",
    "\n",
    "        text = train_prompt_style.format(question, response)\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}"
   ],
   "id": "ff4059100ce88efb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import the datasets library to load and process the training dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the medical reasoning dataset\n",
    "dataset = load_dataset(\n",
    "    \"mamachang/medical-reasoning\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "# Format the dataset using the defined prompt formatting function\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")\n",
    "print(dataset[\"text\"][10])"
   ],
   "id": "2b7487a7b15be0dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import data collator for language modeling tasks\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Define the data collator, disabling masked language modeling (mlm)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ],
   "id": "ab35eed37862a504",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Model Inference Before Fine-Tuning",
   "id": "6972ac24e09f05dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the prompt template for inference\n",
    "inference_prompt_style = \"\"\"\n",
    "Please answer with one of the options in the bracket. Write reasoning in between <analysis></analysis>. Write the answer in between <answer></answer>.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<analysis>\n",
    "\"\"\""
   ],
   "id": "42bfc9cd61e90cc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Select a sample question from the dataset for inference\n",
    "question = dataset[10]['input']\n",
    "question = question.replace(\"Q:\", \"\")\n",
    "\n",
    "# Tokenize the input question and prepare tensors for the model\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt_style.format(question) + tokenizer.eos_token],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate a response from the model\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "# Decode and print the model's response\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ],
   "id": "ac934e821234e542",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import LoRA configuration and model wrapping utilities\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,  # Scaling factor for LoRA\n",
    "    lora_dropout=0.05,  # Add a slight dropout for regularization\n",
    "    r=64,  # Rank of the LoRA update matrices\n",
    "    bias=\"none\",  # No bias reparameterization\n",
    "    task_type=\"CAUSAL_LM\",  # Task type: Causal Language Modeling\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # Target modules for LoRA\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA configuration\n",
    "model = get_peft_model(model, peft_config)"
   ],
   "id": "353cba9c1c5b22fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Setting up the model",
   "id": "63cfe09e4dfc2150"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import the SFTTrainer for supervised fine-tuning\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training Arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"../models/DeepSeek-R1-Distill-Qwen-1.5B-Medical-Reasoning\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not is_bfloat16_supported(), # Use FP16 if BF16 is not supported\n",
    "    bf16=is_bfloat16_supported(),     # Use BF16 if supported\n",
    "    group_by_length=True,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    data_collator=data_collator,\n",
    ")"
   ],
   "id": "295d1b7f46c3b2d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Model Training",
   "id": "1c081675cdd8c283"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Clear GPU memory and disable cache before training\n",
    "import gc, torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Start the training process\n",
    "trainer.train()"
   ],
   "id": "15c10dba217dafac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Model inference after fine-tuning",
   "id": "c464db27e65e941d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate the model inferences after fine-tuning\n",
    "question = dataset[10]['input']\n",
    "question = question.replace(\"Q:\", \"\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt_style.format(question, ) + tokenizer.eos_token],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ],
   "id": "e60ab098601c484d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print the ground truth output for comparison\n",
    "print(dataset[10]['output'])"
   ],
   "id": "d9a0569a9ec0cd77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate the model on a different question from the dataset\n",
    "question = dataset[100]['input']\n",
    "question = question.replace(\"Q:\", \"\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt_style.format(question) + tokenizer.eos_token],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ],
   "id": "dea182fd3e15f4cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print the ground truth output for the second question\n",
    "print(dataset[100]['output'])"
   ],
   "id": "b91c34da226c1c76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Saving the model",
   "id": "ba7a6dd476bbe8db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Push the fine-tuned model and tokenizer to Hugging Face Hub\n",
    "new_model_name = \"DeepSeek-R1-Distill-Qwen-1.5B-Medical-Reasoning\"\n",
    "trainer.model.push_to_hub(new_model_name)\n",
    "trainer.processing_class.push_to_hub(new_model_name)"
   ],
   "id": "69a7f3064e5f19fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. Loading the Adopter and testing the model",
   "id": "9a8062a390039578"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Clean up model and trainer objects, and clear GPU memory\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "94e3cf67af457f1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Re-import necessary libraries for loading the fine-tuned model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Base model\n",
    "base_model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "# Your fine-tuned LoRA adapter repository\n",
    "lora_adapter_id = \"kingabzpro/DeepSeek-R1-Distill-Qwen-1.5B-Medical-Reasoning\"\n",
    "\n",
    "# Load the model in 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Attach the LoRA adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    lora_adapter_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n"
   ],
   "id": "183a8903840855a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Inference example\n",
    "prompt = \"\"\"\n",
    "Please answer with one of the options in the bracket. Write reasoning in between <analysis></analysis>. Write the answer in between <answer></answer>.\n",
    "\n",
    "### Question:\n",
    "A research group wants to assess the relationship between childhood diet and cardiovascular disease in adulthood.\n",
    "A prospective cohort study of 500 children between 10 to 15 years of age is conducted in which the participants' diets are recorded for 1 year and then the patients are assessed 20 years later for the presence of cardiovascular disease.\n",
    "A statistically significant association is found between childhood consumption of vegetables and decreased risk of hyperlipidemia and exercise tolerance.\n",
    "When these findings are submitted to a scientific journal, a peer reviewer comments that the researchers did not discuss the study's validity.\n",
    "Which of the following additional analyses would most likely address the concerns about this study's design?\n",
    "{'A': 'Blinding', 'B': 'Crossover', 'C': 'Matching', 'D': 'Stratification', 'E': 'Randomization'},\n",
    "### Response:\n",
    "<analysis>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the prompt and prepare tensors for the model\n",
    "inputs = tokenizer(\n",
    "    [prompt + tokenizer.eos_token],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate a response from the model\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(response[0].split(\"### Response:\")[1])\n",
    "\n"
   ],
   "id": "4f355a2540e64598",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
