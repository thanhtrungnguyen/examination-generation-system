{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fine-Tuning DeepSeek-R1-0528",
   "id": "1c6fea978d73a465"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Setting Up",
   "id": "461c218f87430b57"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-24T15:59:19.940197Z",
     "start_time": "2025-07-24T15:59:18.585590Z"
    }
   },
   "source": [
    "# Import required libraries for authentication and environment variable management\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get token from environment variable\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "# Log in to Hugging Face Hub using the token from the environment\n",
    "if hf_token:\n",
    "    login(hf_token)\n",
    "    print(\"✅ Hugging Face login successful.\")\n",
    "else:\n",
    "    print(\"❌ HF_TOKEN not found. Please check your .env file.\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hugging Face login successful.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Loading the Model and Tokenizer",
   "id": "29e4ec97c635cf8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:59:23.385680Z",
     "start_time": "2025-07-24T15:59:20.004630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import model and tokenizer classes, and torch for tensor operations\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n"
   ],
   "id": "5a4519f0b35dfb72",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:59:23.495746Z",
     "start_time": "2025-07-24T15:59:23.490551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure 4-bit quantization for efficient model loading\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ],
   "id": "172a6e39abf80d74",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:59:27.945572Z",
     "start_time": "2025-07-24T15:59:23.554481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load tokenizer and model from Hugging Face Hub with quantization settings\n",
    "model_dir = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ],
   "id": "e188e94674721544",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:59:28.087439Z",
     "start_time": "2025-07-24T15:59:27.980268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check GPU availability and status\n",
    "!nvidia-smi"
   ],
   "id": "3e3c2417398de233",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 24 22:59:28 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.88                 Driver Version: 576.88         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   49C    P0             23W /   85W |    2176MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            7484      C   ..._qbz5n2kfra8p0\\python3.13.exe      N/A      |\n",
      "|    0   N/A  N/A           21620    C+G   ...s\\PowerToys.PowerLauncher.exe      N/A      |\n",
      "|    0   N/A  N/A           23752    C+G   ...yApp\\MicrosoftSecurityApp.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Loading and Processing the Dataset",
   "id": "6afd3c259e2cc4c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:59:28.125321Z",
     "start_time": "2025-07-24T15:59:28.122127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the prompt template for training\n",
    "train_prompt_style = \"\"\"\n",
    "Please answer with one of the options in the bracket. Write reasoning in between <analysis></analysis>. Write the answer in between <answer></answer>.\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ],
   "id": "1ba95693b1afb563",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:59:28.165423Z",
     "start_time": "2025-07-24T15:59:28.161566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the end-of-sequence token from the tokenizer\n",
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "\n",
    "# Define a function to format prompts for the model\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for question, response in zip(inputs, outputs):\n",
    "        # Remove the \"Q:\" prefix from the question\n",
    "        question = question.replace(\"Q:\", \"\")\n",
    "\n",
    "        # Append the EOS token to the response if it's not already there\n",
    "        if not response.endswith(tokenizer.eos_token):\n",
    "            response += tokenizer.eos_token\n",
    "\n",
    "        text = train_prompt_style.format(question, response)\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}"
   ],
   "id": "ff4059100ce88efb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:59:33.052144Z",
     "start_time": "2025-07-24T15:59:28.202757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import the datasets library to load and process the training dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the medical reasoning dataset\n",
    "dataset = load_dataset(\n",
    "    \"mamachang/medical-reasoning\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "# Format the dataset using the defined prompt formatting function\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")\n",
    "print(dataset[\"text\"][10])"
   ],
   "id": "2b7487a7b15be0dd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'mamachang/medical-reasoning' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please answer with one of the options in the bracket. Write reasoning in between <analysis></analysis>. Write the answer in between <answer></answer>.\n",
      "### Question:\n",
      "A research group wants to assess the relationship between childhood diet and cardiovascular disease in adulthood. A prospective cohort study of 500 children between 10 to 15 years of age is conducted in which the participants' diets are recorded for 1 year and then the patients are assessed 20 years later for the presence of cardiovascular disease. A statistically significant association is found between childhood consumption of vegetables and decreased risk of hyperlipidemia and exercise tolerance. When these findings are submitted to a scientific journal, a peer reviewer comments that the researchers did not discuss the study's validity. Which of the following additional analyses would most likely address the concerns about this study's design?? \n",
      "{'A': 'Blinding', 'B': 'Crossover', 'C': 'Matching', 'D': 'Stratification', 'E': 'Randomization'},\n",
      "\n",
      "### Response:\n",
      "<analysis>\n",
      "\n",
      "This is a question about assessing the validity of a prospective cohort study. The study found an association between childhood diet and cardiovascular disease in adulthood. The peer reviewer is concerned that the researchers did not discuss the validity of the study design. \n",
      "\n",
      "To address concerns about validity in a prospective cohort study, we need to consider potential confounding factors that could influence the results. The additional analysis suggested should help control for confounding.\n",
      "</analysis>\n",
      "<answer>\n",
      "D: Stratification\n",
      "</answer><｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:59:33.071327Z",
     "start_time": "2025-07-24T15:59:33.059027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import data collator for language modeling tasks\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Define the data collator, disabling masked language modeling (mlm)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ],
   "id": "ab35eed37862a504",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Model Inference Before Fine-Tuning",
   "id": "6972ac24e09f05dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:59:33.129225Z",
     "start_time": "2025-07-24T15:59:33.126210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the prompt template for inference\n",
    "inference_prompt_style = \"\"\"\n",
    "Please answer with one of the options in the bracket. Write reasoning in between <analysis></analysis>. Write the answer in between <answer></answer>.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<analysis>\n",
    "\"\"\""
   ],
   "id": "42bfc9cd61e90cc7",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:59:37.241804Z",
     "start_time": "2025-07-24T15:59:33.164674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select a sample question from the dataset for inference\n",
    "question = dataset[10]['input']\n",
    "question = question.replace(\"Q:\", \"\")\n",
    "\n",
    "# Tokenize the input question and prepare tensors for the model\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt_style.format(question) + tokenizer.eos_token],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate a response from the model\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "# Decode and print the model's response\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ],
   "id": "ac934e821234e542",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<analysis>\n",
      "\n",
      "</think>\n",
      "\n",
      "<analysis></analysis>  \n",
      "In the context of the study described, the peer reviewer's comment about the study's validity primarily concerns the design of the research. A prospective cohort study is well-designed, but it is crucial to ensure that the participants are adequately randomized to prevent confounding and bias. Randomization is a key component of the design to ensure that the study results are generalizable and reliable.  \n",
      "\n",
      "**Answer:</answer>  \n",
      "E: Randomization\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:59:37.863186Z",
     "start_time": "2025-07-24T15:59:37.299714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import LoRA configuration and model wrapping utilities\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,  # Scaling factor for LoRA\n",
    "    lora_dropout=0.05,  # Add a slight dropout for regularization\n",
    "    r=64,  # Rank of the LoRA update matrices\n",
    "    bias=\"none\",  # No bias reparameterization\n",
    "    task_type=\"CAUSAL_LM\",  # Task type: Causal Language Modeling\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # Target modules for LoRA\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA configuration\n",
    "model = get_peft_model(model, peft_config)"
   ],
   "id": "353cba9c1c5b22fc",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Setting up the model",
   "id": "63cfe09e4dfc2150"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T16:09:04.931036Z",
     "start_time": "2025-07-24T16:09:02.566340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import the SFTTrainer for supervised fine-tuning\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training Arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"../models/DeepSeek-R1-Distill-Qwen-1.5B-Medical-Reasoning\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    logging_dir=\"../../logs/\"\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    data_collator=data_collator,\n",
    ")"
   ],
   "id": "295d1b7f46c3b2d6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Model Training",
   "id": "1c081675cdd8c283"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-07-24T15:59:39.844551Z",
     "start_time": "2025-07-24T13:20:45.169760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Clear GPU memory and disable cache before training\n",
    "import gc, torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Start the training process\n",
    "trainer.train()"
   ],
   "id": "15c10dba217dafac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='426' max='1851' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 426/1851 22:45 < 1:16:30, 0.31 it/s, Epoch 0.23/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>1.654900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Model inference after fine-tuning",
   "id": "c464db27e65e941d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate the model inferences after fine-tuning\n",
    "question = dataset[10]['input']\n",
    "question = question.replace(\"Q:\", \"\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt_style.format(question, ) + tokenizer.eos_token],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ],
   "id": "e60ab098601c484d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print the ground truth output for comparison\n",
    "print(dataset[10]['output'])"
   ],
   "id": "d9a0569a9ec0cd77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate the model on a different question from the dataset\n",
    "question = dataset[100]['input']\n",
    "question = question.replace(\"Q:\", \"\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt_style.format(question) + tokenizer.eos_token],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ],
   "id": "dea182fd3e15f4cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print the ground truth output for the second question\n",
    "print(dataset[100]['output'])"
   ],
   "id": "b91c34da226c1c76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Saving the model",
   "id": "ba7a6dd476bbe8db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Push the fine-tuned model and tokenizer to Hugging Face Hub\n",
    "new_model_name = \"DeepSeek-R1-Distill-Qwen-1.5B-Medical-Reasoning\"\n",
    "trainer.model.push_to_hub(new_model_name)\n",
    "trainer.processing_class.push_to_hub(new_model_name)"
   ],
   "id": "69a7f3064e5f19fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. Loading the Adopter and testing the model",
   "id": "9a8062a390039578"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Clean up model and trainer objects, and clear GPU memory\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "94e3cf67af457f1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Re-import necessary libraries for loading the fine-tuned model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Base model\n",
    "base_model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "# Your fine-tuned LoRA adapter repository\n",
    "lora_adapter_id = \"kingabzpro/DeepSeek-R1-Distill-Qwen-1.5B-Medical-Reasoning\"\n",
    "\n",
    "# Load the model in 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Attach the LoRA adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    lora_adapter_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n"
   ],
   "id": "183a8903840855a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Inference example\n",
    "prompt = \"\"\"\n",
    "Please answer with one of the options in the bracket. Write reasoning in between <analysis></analysis>. Write the answer in between <answer></answer>.\n",
    "\n",
    "### Question:\n",
    "A research group wants to assess the relationship between childhood diet and cardiovascular disease in adulthood.\n",
    "A prospective cohort study of 500 children between 10 to 15 years of age is conducted in which the participants' diets are recorded for 1 year and then the patients are assessed 20 years later for the presence of cardiovascular disease.\n",
    "A statistically significant association is found between childhood consumption of vegetables and decreased risk of hyperlipidemia and exercise tolerance.\n",
    "When these findings are submitted to a scientific journal, a peer reviewer comments that the researchers did not discuss the study's validity.\n",
    "Which of the following additional analyses would most likely address the concerns about this study's design?\n",
    "{'A': 'Blinding', 'B': 'Crossover', 'C': 'Matching', 'D': 'Stratification', 'E': 'Randomization'},\n",
    "### Response:\n",
    "<analysis>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the prompt and prepare tensors for the model\n",
    "inputs = tokenizer(\n",
    "    [prompt + tokenizer.eos_token],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate a response from the model\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(response[0].split(\"### Response:\")[1])\n",
    "\n"
   ],
   "id": "4f355a2540e64598",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
