{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-29T07:40:38.360308Z",
     "start_time": "2025-07-29T07:40:36.616383Z"
    }
   },
   "source": "!pip install -q -U bitsandbytes  datasets evaluate accelerate peft transformers trl",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T07:40:39.912639Z",
     "start_time": "2025-07-29T07:40:38.485727Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -q -U langchain",
   "id": "7033ef355941312c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T07:42:14.280317Z",
     "start_time": "2025-07-29T07:40:39.922298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, LoftQConfig\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import gc, torch\n",
    "from evaluate import load\n",
    "\n",
    "# Load dataset (SQuAD example)\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "# Define LangChain prompt template\n",
    "qa_template = \"\"\"### Question: {question}\n",
    "### Context: {context}\n",
    "### Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=qa_template\n",
    ")\n",
    "\n",
    "# Preprocessing function with LangChain template\n",
    "def preprocess_function(examples):\n",
    "    questions = examples['question']\n",
    "    contexts = examples['context']\n",
    "    answers = [a['text'][0] for a in examples['answers']]\n",
    "\n",
    "    input_prompts = []\n",
    "    full_texts = []\n",
    "    for q, c, a in zip(questions, contexts, answers):\n",
    "        input_prompt = prompt_template.format(question=q, context=c)\n",
    "        full_text = input_prompt + a\n",
    "        input_prompts.append(input_prompt)\n",
    "        full_texts.append(full_text)\n",
    "\n",
    "    # Tokenization\n",
    "    tokenized_data = tokenizer(\n",
    "        full_texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Create labels by masking input prompts\n",
    "    labels = tokenized_data[\"input_ids\"].clone()\n",
    "    for i, input_prompt in enumerate(input_prompts):\n",
    "        prompt_length = len(tokenizer(input_prompt, return_tensors=\"pt\").input_ids[0])\n",
    "        labels[i, :prompt_length-1] = -100\n",
    "\n",
    "    return {\n",
    "        'input_ids': tokenized_data['input_ids'],\n",
    "        'attention_mask': tokenized_data['attention_mask'],\n",
    "        'labels': labels,\n",
    "        'question': questions,\n",
    "        'context': contexts,\n",
    "        'answers': answers,\n",
    "        'id': examples['id']\n",
    "    }\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "loftq_config = LoftQConfig(loftq_bits=4)           # set 4bit quantization\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "\n",
    "# Modified LoRA config to target all layer types\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Process dataset with preserved columns for LangChain\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=['title', 'answers']  # Preserve original question/context\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/deepseek-r1-finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model.config.use_cache = False\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save adapter\n",
    "model.save_pretrained(\"deepseek-r1-1.5B-squad-lora\")\n",
    "\n",
    "# Create LangChain pipeline for evaluation\n",
    "model.eval()\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=model.device.index,\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    no_repeat_ngram_size=2,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "qa_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# QA Evaluation using LangChain\n",
    "squad_metric = load(\"squad\")\n",
    "tokenized_val = tokenized_dataset[\"validation\"]\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for example in tokenized_val:\n",
    "    response = qa_chain.run(\n",
    "        question=example[\"question\"],\n",
    "        context=example[\"context\"]\n",
    "    )\n",
    "    answer = response.split(\"### Answer: \")[-1].strip()\n",
    "\n",
    "    predictions.append({\n",
    "        \"prediction_text\": answer,\n",
    "        \"id\": example[\"id\"]\n",
    "    })\n",
    "\n",
    "    references.append({\n",
    "        \"answers\": {\"text\": [example[\"answers\"]], \"answer_start\": [0]},\n",
    "        \"id\": example[\"id\"]\n",
    "    })\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = squad_metric.compute(predictions=predictions, references=references)\n",
    "print(f\"\\nFinal Evaluation Metrics:\")\n",
    "print(f\"Exact Match: {metrics['exact_match']:.2f}%\")\n",
    "print(f\"F1 Score: {metrics['f1']:.2f}%\")"
   ],
   "id": "3ad14486e52a5ea6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,358,144 || all params: 1,781,446,144 || trainable%: 0.2446\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d8836a8e87b41879ca8d5579bcc5e41"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e61defe83c042c9947ca1ed8b01964e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNotImplementedError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 127\u001B[39m\n\u001B[32m    104\u001B[39m training_args = TrainingArguments(\n\u001B[32m    105\u001B[39m     output_dir=\u001B[33m\"\u001B[39m\u001B[33m../models/deepseek-r1-finetuned\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    106\u001B[39m     num_train_epochs=\u001B[32m3\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    123\u001B[39m     report_to=\u001B[33m\"\u001B[39m\u001B[33mnone\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    124\u001B[39m )\n\u001B[32m    126\u001B[39m \u001B[38;5;66;03m# Initialize Trainer\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m127\u001B[39m trainer = \u001B[43mTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    128\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    129\u001B[39m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtraining_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    130\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenized_dataset\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtrain\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    131\u001B[39m \u001B[43m    \u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenized_dataset\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mvalidation\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    132\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata_collator\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdefault_data_collator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    133\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m    134\u001B[39m gc.collect()\n\u001B[32m    135\u001B[39m torch.cuda.empty_cache()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\examination-generation-system\\exam_generator\\.venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001B[39m, in \u001B[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    168\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[32m    169\u001B[39m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[32m    170\u001B[39m     warnings.warn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel=\u001B[32m2\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m172\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\examination-generation-system\\exam_generator\\.venv\\Lib\\site-packages\\transformers\\trainer.py:620\u001B[39m, in \u001B[36mTrainer.__init__\u001B[39m\u001B[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001B[39m\n\u001B[32m    615\u001B[39m \u001B[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001B[39;00m\n\u001B[32m    616\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    617\u001B[39m     \u001B[38;5;28mself\u001B[39m.place_model_on_device\n\u001B[32m    618\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(model, \u001B[33m\"\u001B[39m\u001B[33mquantization_method\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) == QuantizationMethod.BITS_AND_BYTES\n\u001B[32m    619\u001B[39m ):\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_move_model_to_device\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001B[39;00m\n\u001B[32m    623\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.is_model_parallel:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\examination-generation-system\\exam_generator\\.venv\\Lib\\site-packages\\transformers\\trainer.py:901\u001B[39m, in \u001B[36mTrainer._move_model_to_device\u001B[39m\u001B[34m(self, model, device)\u001B[39m\n\u001B[32m    900\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_move_model_to_device\u001B[39m(\u001B[38;5;28mself\u001B[39m, model, device):\n\u001B[32m--> \u001B[39m\u001B[32m901\u001B[39m     model = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    902\u001B[39m     \u001B[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001B[39;00m\n\u001B[32m    903\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.parallel_mode == ParallelMode.TPU \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(model, \u001B[33m\"\u001B[39m\u001B[33mtie_weights\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\examination-generation-system\\exam_generator\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1355\u001B[39m, in \u001B[36mModule.to\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1352\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1353\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1355\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\examination-generation-system\\exam_generator\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    913\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    914\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m915\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    917\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[32m    918\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    919\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    920\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    925\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    926\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\examination-generation-system\\exam_generator\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    913\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    914\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m915\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    917\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[32m    918\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    919\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    920\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    925\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    926\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[31m[... skipping similar frames: Module._apply at line 915 (5 times)]\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\examination-generation-system\\exam_generator\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    913\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    914\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m915\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    917\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[32m    918\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    919\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    920\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    925\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    926\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\examination-generation-system\\exam_generator\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:942\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    938\u001B[39m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[32m    939\u001B[39m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[32m    940\u001B[39m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[32m    941\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m--> \u001B[39m\u001B[32m942\u001B[39m     param_applied = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    943\u001B[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001B[32m    945\u001B[39m \u001B[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\examination-generation-system\\exam_generator\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1348\u001B[39m, in \u001B[36mModule.to.<locals>.convert\u001B[39m\u001B[34m(t)\u001B[39m\n\u001B[32m   1346\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1347\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) == \u001B[33m\"\u001B[39m\u001B[33mCannot copy out of meta tensor; no data!\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m1348\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[32m   1349\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1350\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mwhen moving module from meta to a different device.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1351\u001B[39m         ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1352\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1353\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[31mNotImplementedError\u001B[39m: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device."
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
