{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# DeepSeek-R1-Distill-Qwen-1.5B-SciQ\n",
    "## Fine-Tuning DeepSeek-R1-Distill-Qwen-1.5B\n",
    "This notebook demonstrates how to fine-tune the DeepSeek-R1-Distill-Qwen-1.5B model on the SciQ dataset for multiple-choice question answering (MCQA) tasks. The process includes setting up the environment, loading the model and tokenizer, processing the dataset, training the model, and evaluating its performance.\n",
    "\n",
    "[![Open in Colab](https://img.shields.io/badge/Open%20in-Colab-orange?logo=google&logoColor=white)](https://colab.research.google.com/drive/1GVlPxUok2vym4Yku1-_tBmcA-STb-ouq?usp=sharing) [![Open in Kaggle](https://img.shields.io/badge/Open%20in-Kaggle-blue?logo=kaggle&logoColor=white)](https://www.kaggle.com/code/trungngthanh/deepseek-r1-distill-qwen-1-5b-mcqa)"
   ],
   "id": "1c6fea978d73a465"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Setting Up",
   "id": "461c218f87430b57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "def check_environment():\n",
    "    # Check for Google Colab\n",
    "    if 'google.colab' in sys.modules:\n",
    "        return \"Google Colab\"\n",
    "\n",
    "    # Check for Kaggle\n",
    "    if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "        return \"Kaggle\"\n",
    "\n",
    "    # Check for local machine\n",
    "    if os.path.exists('/content') and 'COLAB_GPU' in os.environ:\n",
    "        # Double-check Colab (some Colab envs may not import google.colab)\n",
    "        return \"Google Colab\"\n",
    "    else:\n",
    "        # Assume local if neither Colab nor Kaggle\n",
    "        return \"Local Machine\"\n",
    "\n",
    "# Detect and print the current environment and system information\n",
    "env = check_environment()\n",
    "print(f\"Running on: {env}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")"
   ],
   "id": "84942604677b9e69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Install required packages based on the detected environment (Kaggle, Colab, or local)\n",
    "if env == \"Kaggle\":\n",
    "    %pip install -U transformers\n",
    "    %pip install -U datasets\n",
    "    %pip install -U accelerate\n",
    "    %pip install -U peft\n",
    "    %pip install -U trl\n",
    "    %pip install -U bitsandbytes\n",
    "elif env == \"Google Colab\":\n",
    "    %pip install python-dotenv\n",
    "    %pip install -U transformers\n",
    "    %pip install -U datasets\n",
    "    %pip install -U accelerate\n",
    "    %pip install -U peft\n",
    "    %pip install -U trl\n",
    "    %pip install -U bitsandbytes\n",
    "else:\n",
    "    print(\"Please install the required packages manually for local execution.\")"
   ],
   "id": "d8e1bfcc9cfa7eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import libraries for Hugging Face authentication and environment variable management\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file (if present)\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = ''\n",
    "# Retrieve Hugging Face token from the appropriate source depending on environment\n",
    "if env == \"Kaggle\":\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        user_secrets = UserSecretsClient()\n",
    "        hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "        print(\"HF_TOKEN is set in Kaggle Secrets\")\n",
    "    except:\n",
    "        print(\"HF_TOKEN is not set in Kaggle Secrets\")\n",
    "elif env == \"Google Colab\":\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "        print(\"HF_TOKEN is set in Colab .env file\")\n",
    "    except ImportError:\n",
    "        print(\"python-dotenv not installed in Colab\")\n",
    "else:\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "        print(\"HF_TOKEN is set in local .env file\")\n",
    "    except ImportError:\n",
    "        print(\"python-dotenv not installed. Please install it to manage environment variables.\")\n",
    "\n",
    "# Log in to Hugging Face Hub using the token from the environment (if available)\n",
    "if hf_token:\n",
    "    login(hf_token)\n",
    "    print(\"✅ Hugging Face login successful.\")\n",
    "else:\n",
    "    print(\"❌ HF_TOKEN not found. Please check your .env file.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "e371f31a140bd923",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check GPU availability and status\n",
    "!nvidia-smi"
   ],
   "id": "a2e174033e1c5a12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Loading the Model and Tokenizer",
   "id": "29e4ec97c635cf8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import model and tokenizer classes for tensor operations\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n"
   ],
   "id": "5a4519f0b35dfb72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configure 4-bit quantization for efficient model loading\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ],
   "id": "172a6e39abf80d74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load tokenizer and model from Hugging Face Hub with quantization settings\n",
    "model_dir = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ],
   "id": "e188e94674721544",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Loading and Processing the Dataset",
   "id": "6afd3c259e2cc4c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "def format_sciq_prompt(example, num_questions=1, include_cot=False):\n",
    "    # Extract context and question details\n",
    "    context = example['support']\n",
    "    question = example['question']\n",
    "    options = [example['distractor1'], example['distractor2'], example['distractor3'], example['correct_answer']]\n",
    "    random.shuffle(options)\n",
    "    # Assign correct answer to one of the options (e.g., D)\n",
    "    correct_answer = example['correct_answer']\n",
    "    option_labels = ['A', 'B', 'C', 'D']\n",
    "    correct_label = chr(65 + options.index(example['correct_answer'])) # Convert index to label (A, B, C, D)\n",
    "\n",
    "    # Create the prompt\n",
    "    prompt = (\n",
    "        f\"Given the context: {context}\\n\"\n",
    "        f\"Generate {num_questions} multiple-choice question(s) with four options each and indicate the correct answer.\\n\"\n",
    "    )\n",
    "\n",
    "    # Format the completion\n",
    "    completion = (\n",
    "        f\"Question: {question}?\\n\"\n",
    "        f\"{option_labels[0]}) {options[0]}\\n\"\n",
    "        f\"{option_labels[1]}) {options[1]}\\n\"\n",
    "        f\"{option_labels[2]}) {options[2]}\\n\"\n",
    "        f\"{option_labels[3]}) {options[3]}\\n\"\n",
    "        f\"Correct Answer: {correct_label}\"\n",
    "    )\n",
    "\n",
    "    # If include_cot is True, add a reasoning explanation (example placeholder)\n",
    "    if include_cot:\n",
    "        cot_explanation = (\n",
    "            f\"\\nExplanation: The correct answer is {correct_answer} because it directly corresponds to the information provided in the context.\"\n",
    "        )\n",
    "        completion += cot_explanation\n",
    "\n",
    "    # If num_questions > 1, assume additional questions are provided or need to be generated\n",
    "    # For simplicity, this example uses the single question from SciQ\n",
    "    # Augmentation for multiple questions would require external generation (see notes below)\n",
    "    if num_questions > 1:\n",
    "        completion += (\n",
    "            \"\\n\\nNote: Additional questions would be generated here based on the context, \"\n",
    "            \"each with four options and a correct answer.\"\n",
    "        )\n",
    "\n",
    "    return {\"prompt\": prompt, \"completion\": completion}"
   ],
   "id": "879ab6f572c6aa88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import the datasets library to load and process the training dataset\n",
    "from datasets import load_dataset\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load the medical reasoning dataset\n",
    "train_dataset = load_dataset(\n",
    "    \"allenai/sciq\",\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "# FIXME: Take first 10 samples for training\n",
    "# Take first 10 samples for training\n",
    "# train_dataset = train_dataset.select(range(10))"
   ],
   "id": "2b7487a7b15be0dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(train_dataset)\n",
    "df.head()"
   ],
   "id": "9d7b2771b6a4d0f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Format the dataset using the defined prompt formatting function\n",
    "formatted_train_dataset = train_dataset.map(format_sciq_prompt)"
   ],
   "id": "488e6d566cd52a98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Prompt:\\n\" + formatted_train_dataset[0]['prompt'])\n",
    "print(\"Completion:\\n\" + formatted_train_dataset[0]['completion'])"
   ],
   "id": "481d06efc29923b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def tokenize_example(batch):\n",
    "    texts = [f\"{prompt}\\n{completion}{tokenizer.eos_token}\" for prompt, completion in zip(batch['prompt'], batch['completion'])]\n",
    "    return tokenizer(texts, truncation=True, max_length=512, padding=\"max_length\")"
   ],
   "id": "ab35eed37862a504",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenized_dataset = formatted_train_dataset.map(tokenize_example, batched=True, remove_columns=[\"prompt\", \"completion\"])",
   "id": "a4ece161738e8fa6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import data collator for language modeling tasks\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Define the data collator, disabling masked language modeling (mlm)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ],
   "id": "1798d889e66bfb88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Model Inference Before Fine-Tuning",
   "id": "6972ac24e09f05dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# inference_prompt_style = (\n",
    "#     \"Given the context: {}.\\n\"\n",
    "#     \"Generate 1 multiple-choice question with four options and indicate the correct answer.\"\n",
    "# )"
   ],
   "id": "9063910595df1d84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_mcqa(prompt, model, tokenizer, max_length=1000):\n",
    "    inputs = tokenizer(\n",
    "        [prompt+ tokenizer.eos_token],\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=max_length,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ],
   "id": "ac934e821234e542",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prompt = formatted_train_dataset[0]['prompt']\n",
    "print(prompt)\n",
    "generated = generate_mcqa(prompt, model, tokenizer)"
   ],
   "id": "8f083bdf21ff9400",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(generated)",
   "id": "9e4b38aee8dc909b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Inference Before Fine-Tuning:\")\n",
    "print(generated[0])"
   ],
   "id": "353cba9c1c5b22fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Setting up the model",
   "id": "63cfe09e4dfc2150"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import LoRA configuration and model wrapping utilities\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,  # Scaling factor for LoRA\n",
    "    lora_dropout=0.1,  # Add a slight dropout for regularization\n",
    "    r=4,  # Rank of the LoRA update matrices\n",
    "    # use_rslora=True,  # Stabilize scaling\n",
    "    # bias=\"none\",  # No bias reparameterization\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    task_type=\"CAUSAL_LM\",  # Task type: Causal Language Modeling\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        # \"gate_proj\",\n",
    "        # \"up_proj\",\n",
    "        # \"down_proj\",\n",
    "    ],  # Target modules for LoRA\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA configuration\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()  # Verify ~0.5% trainable params"
   ],
   "id": "3bb1acc53a92c778",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import the SFTTrainer for supervised fine-tuning\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training Arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"../models/DeepSeek-R1-Distill-Qwen-1.5B-SciQ\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    save_steps=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=1e-4,\n",
    "    fp16=False,  # Use FP16 if BF16 is not supported\n",
    "    bf16=False,      # Use BF16 if supported\n",
    "    group_by_length=True,\n",
    "    report_to=\"tensorboard\",  # tensorboard, wandb, etc. can be used here\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    peft_config=peft_config,\n",
    "    data_collator=data_collator,\n",
    ")"
   ],
   "id": "9216c2cded2c655b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Model Training",
   "id": "1c081675cdd8c283"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ../models/DeepSeek-R1-Distill-Qwen-1.5B-SciQ/runs  # Point to your output_dir/runs"
   ],
   "id": "8f04c5a2c6930665",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Clear GPU memory and disable cache before training\n",
    "import gc, torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Start the training process\n",
    "trainer.train()"
   ],
   "id": "15c10dba217dafac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Model inference after fine-tuning",
   "id": "c464db27e65e941d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate the model inferences after fine-tuning\n",
    "prompt = formatted_train_dataset[0]['prompt']\n",
    "generated = generate_mcqa(prompt, model, tokenizer)\n",
    "print(\"Inference After Fine-Tuning:\")\n",
    "print(generated[0])"
   ],
   "id": "e60ab098601c484d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Saving the model",
   "id": "ba7a6dd476bbe8db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Push the fine-tuned model and tokenizer to Hugging Face Hub\n",
    "new_model_name = \"DeepSeek-R1-Distill-Qwen-1.5B-SciQ\"\n",
    "trainer.model.push_to_hub(new_model_name)\n",
    "trainer.processing_class.push_to_hub(new_model_name)"
   ],
   "id": "69a7f3064e5f19fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. Loading the Adopter and testing the model",
   "id": "9a8062a390039578"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Clean up model and trainer objects, and clear GPU memory\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "94e3cf67af457f1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from huggingface_hub import whoami\n",
    "\n",
    "# Get the current user's information\n",
    "user_info = whoami()\n",
    "\n",
    "# Extract the username\n",
    "hf_user = user_info['name']\n",
    "print(f\"Username: {hf_user}\")"
   ],
   "id": "141d26f917a148f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Re-import necessary libraries for loading the fine-tuned model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Base model\n",
    "base_model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "# Your fine-tuned LoRA adapter repository\n",
    "lora_adapter_id = f\"{hf_user}/DeepSeek-R1-Distill-Qwen-1.5B-SciQ\"\n",
    "\n",
    "# Load the model in 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Attach the LoRA adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    lora_adapter_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)"
   ],
   "id": "183a8903840855a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the medical reasoning dataset\n",
    "test_dataset = load_dataset(\n",
    "    \"allenai/sciq\",\n",
    "    split=\"test\",\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.select(range(20))\n",
    "\n",
    "formatted_test_dataset = test_dataset.map(format_sciq_prompt)"
   ],
   "id": "2300bd94111f9375",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define a function to generate the response for each record\n",
    "def add_generated_response(example):\n",
    "    prompt = format_sciq_prompt(example, num_questions=1, include_cot=False)['prompt']\n",
    "    generated = generate_mcqa(prompt, model, tokenizer)\n",
    "    example['generated_response'] = generated[0]\n",
    "    return example\n",
    "\n",
    "# Use map to add the new column to the dataset\n",
    "testing_set = test_dataset.map(add_generated_response)"
   ],
   "id": "f56afcb0086f4fe5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert the test set to a DataFrame and show the first 5 records with generated responses\n",
    "df = pd.DataFrame(testing_set)\n",
    "df.head()"
   ],
   "id": "64101f37ca95a4e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "if env == \"Google Colab\":\n",
    "\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Get current timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    # Save the testing set with generated responses to a CSV file\n",
    "    output_file = f\"sciq_test_with_responses_{timestamp}.csv\"\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Testing set with generated responses saved to {output_file}\")\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    output_dir = \"/content/drive/My Drive/colab_outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Move the file\n",
    "    !mv {output_file} \"{output_dir}/\"\n",
    "else:\n",
    "    # Save output file\n",
    "    # Get current timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    # Save the testing set with generated responses to a CSV file\n",
    "    output_file = f\"sciq_test_with_responses_{timestamp}.csv\"\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n"
   ],
   "id": "fb35c77dbe6b373f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e8db450e77df41e8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
