{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# DeepSeek-R1-Distill-Qwen-1.5B-SciQ\n",
    "## Fine-Tuning DeepSeek-R1-Distill-Qwen-1.5B\n",
    "This notebook demonstrates how to fine-tune the DeepSeek-R1-Distill-Qwen-1.5B model on the SciQ dataset for multiple-choice question answering (MCQA) tasks. The process includes setting up the environment, loading the model and tokenizer, processing the dataset, training the model, and evaluating its performance.\n",
    "\n",
    "[![Open in Colab](https://img.shields.io/badge/Open%20in-Colab-orange?logo=google&logoColor=white)](https://colab.research.google.com/drive/1GVlPxUok2vym4Yku1-_tBmcA-STb-ouq?usp=sharing)"
   ],
   "id": "1c6fea978d73a465"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Setting Up",
   "id": "461c218f87430b57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "def check_environment():\n",
    "    # Check for Google Colab\n",
    "    if 'google.colab' in sys.modules:\n",
    "        return \"Google Colab\"\n",
    "\n",
    "    # Check for Kaggle\n",
    "    if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "        return \"Kaggle\"\n",
    "\n",
    "    # Check for local machine\n",
    "    if os.path.exists('/content') and 'COLAB_GPU' in os.environ:\n",
    "        # Double-check Colab (some Colab envs may not import google.colab)\n",
    "        return \"Google Colab\"\n",
    "    else:\n",
    "        # Assume local if neither Colab nor Kaggle\n",
    "        return \"Local Machine\"\n",
    "\n",
    "# Print environment details\n",
    "env = check_environment()\n",
    "print(f\"Running on: {env}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "\n",
    "# Check specific environment variables (e.g., from .env or Kaggle Secrets)\n",
    "if env == \"Kaggle\":\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        user_secrets = UserSecretsClient()\n",
    "        api_key = user_secrets.get_secret(\"API_KEY\")\n",
    "        print(\"API_KEY is set in Kaggle Secrets\")\n",
    "    except:\n",
    "        print(\"API_KEY is not set in Kaggle Secrets\")\n",
    "elif env == \"Google Colab\":\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        print(\"API_KEY is set in Colab .env\" if 'API_KEY' in os.environ else \"API_KEY is not set in Colab .env\")\n",
    "    except ImportError:\n",
    "        print(\"python-dotenv not installed in Colab\")\n",
    "else:\n",
    "    print(\"API_KEY is set locally\" if 'API_KEY' in os.environ else \"API_KEY is not set locally\")"
   ],
   "id": "84942604677b9e69"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:59:45.901335Z",
     "start_time": "2025-07-30T09:59:44.368753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "db484161c015ad85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.7.1+cu128\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-30T09:59:46.647794Z",
     "start_time": "2025-07-30T09:59:46.023665Z"
    }
   },
   "source": [
    "# Import required libraries for authentication and environment variable management\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get token from environment variable\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "# Log in to Hugging Face Hub using the token from the environment\n",
    "if hf_token:\n",
    "    login(hf_token)\n",
    "    print(\"✅ Hugging Face login successful.\")\n",
    "else:\n",
    "    print(\"❌ HF_TOKEN not found. Please check your .env file.\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hugging Face login successful.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:59:46.781346Z",
     "start_time": "2025-07-30T09:59:46.659973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check GPU availability and status\n",
    "!nvidia-smi"
   ],
   "id": "a2e174033e1c5a12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 30 16:59:46 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 577.00                 Driver Version: 577.00         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   46C    P8              3W /   87W |     694MiB /   8188MiB |      1%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2108    C+G   ...yApp\\MicrosoftSecurityApp.exe      N/A      |\n",
      "|    0   N/A  N/A            4440    C+G   ...UI3Apps\\PowerToys.Peek.UI.exe      N/A      |\n",
      "|    0   N/A  N/A            9976    C+G   ...em32\\ApplicationFrameHost.exe      N/A      |\n",
      "|    0   N/A  N/A           12064    C+G   ...2txyewy\\CrossDeviceResume.exe      N/A      |\n",
      "|    0   N/A  N/A           12340    C+G   ...y\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           13212    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A           13236    C+G   ...indows\\System32\\ShellHost.exe      N/A      |\n",
      "|    0   N/A  N/A           13456    C+G   ...IA App\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A           14332    C+G   ..._cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A           15220    C+G   ...0_x64__8j3eq9eme6ctt\\IGCC.exe      N/A      |\n",
      "|    0   N/A  N/A           15400    C+G   ...IA App\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A           16448    C+G   ...mba6cd70vzyy\\ArmouryCrate.exe      N/A      |\n",
      "|    0   N/A  N/A           16724    C+G   ...0.3351.109\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           17328    C+G   ...0.3351.109\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           18312    C+G   ...ntrolPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A           19044    C+G   ...Toys\\PowerToys.FancyZones.exe      N/A      |\n",
      "|    0   N/A  N/A           21408    C+G   ...5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A           21816    C+G   ...s\\PowerToys.ColorPickerUI.exe      N/A      |\n",
      "|    0   N/A  N/A           21936    C+G   ...8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           22120    C+G   ...8bbwe\\Microsoft.CmdPal.UI.exe      N/A      |\n",
      "|    0   N/A  N/A           22708    C+G   ...s\\PowerToys.PowerLauncher.exe      N/A      |\n",
      "|    0   N/A  N/A           23620    C+G   ...25.1.2\\jbr\\bin\\cef_server.exe      N/A      |\n",
      "|    0   N/A  N/A           25764    C+G   ...crosoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A           25880    C+G   ...t\\Edge\\Application\\msedge.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Loading the Model and Tokenizer",
   "id": "29e4ec97c635cf8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:59:49.211123Z",
     "start_time": "2025-07-30T09:59:46.788352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import model and tokenizer classes for tensor operations\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n"
   ],
   "id": "5a4519f0b35dfb72",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:59:49.227867Z",
     "start_time": "2025-07-30T09:59:49.222284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure 4-bit quantization for efficient model loading\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ],
   "id": "172a6e39abf80d74",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:59:55.415924Z",
     "start_time": "2025-07-30T09:59:49.237225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load tokenizer and model from Hugging Face Hub with quantization settings\n",
    "model_dir = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ],
   "id": "e188e94674721544",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Loading and Processing the Dataset",
   "id": "6afd3c259e2cc4c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:59:55.459971Z",
     "start_time": "2025-07-30T09:59:55.453348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_sciq_prompt(example, num_questions=1, include_cot=False):\n",
    "    # Extract context and question details\n",
    "    context = example['support']\n",
    "    question = example['question']\n",
    "    options = [example['distractor1'], example['distractor2'], example['distractor3'], example['correct_answer']]\n",
    "    # Assign correct answer to one of the options (e.g., D)\n",
    "    correct_answer = example['correct_answer']\n",
    "    option_labels = ['A', 'B', 'C', 'D']\n",
    "    correct_label = 'D'  # Assuming correct_answer is the last option for consistency\n",
    "\n",
    "    # Create the prompt\n",
    "    prompt = (\n",
    "        f\"Given the context: {context}\\n\"\n",
    "        f\"Generate {num_questions} multiple-choice question(s) with four options each and indicate the correct answer.\"\n",
    "    )\n",
    "\n",
    "    # Format the completion\n",
    "    completion = (\n",
    "        f\"Question: {question}?\\n\"\n",
    "        f\"{option_labels[0]}) {options[0]}\\n\"\n",
    "        f\"{option_labels[1]}) {options[1]}\\n\"\n",
    "        f\"{option_labels[2]}) {options[2]}\\n\"\n",
    "        f\"{option_labels[3]}) {options[3]}\\n\"\n",
    "        f\"Correct Answer: {correct_label}\"\n",
    "    )\n",
    "\n",
    "    # If include_cot is True, add a reasoning explanation (example placeholder)\n",
    "    if include_cot:\n",
    "        cot_explanation = (\n",
    "            f\"\\nExplanation: The correct answer is {correct_answer} because it directly corresponds to the information provided in the context.\"\n",
    "        )\n",
    "        completion += cot_explanation\n",
    "\n",
    "    # If num_questions > 1, assume additional questions are provided or need to be generated\n",
    "    # For simplicity, this example uses the single question from SciQ\n",
    "    # Augmentation for multiple questions would require external generation (see notes below)\n",
    "    if num_questions > 1:\n",
    "        completion += (\n",
    "            \"\\n\\nNote: Additional questions would be generated here based on the context, \"\n",
    "            \"each with four options and a correct answer.\"\n",
    "        )\n",
    "\n",
    "    return {\"prompt\": prompt, \"completion\": completion}"
   ],
   "id": "879ab6f572c6aa88",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T10:00:04.452715Z",
     "start_time": "2025-07-30T09:59:55.502206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import the datasets library to load and process the training dataset\n",
    "from datasets import load_dataset\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load the medical reasoning dataset\n",
    "dataset = load_dataset(\n",
    "    \"allenai/sciq\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# FIXME: Take first 10 samples for training\n",
    "# Take first 10 samples for training\n",
    "dataset = dataset.select(range(10))"
   ],
   "id": "2b7487a7b15be0dd",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T10:00:04.525235Z",
     "start_time": "2025-07-30T10:00:04.503038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "df.head()"
   ],
   "id": "9d7b2771b6a4d0f8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                            question      distractor3  \\\n",
       "0  What type of organism is commonly used in prep...          viruses   \n",
       "1  What phenomenon makes global winds blow northe...  tropical effect   \n",
       "2  Changes from a less-ordered state to a more-or...      endothermic   \n",
       "3     What is the least dangerous radioactive decay?       zeta decay   \n",
       "4  Kilauea in hawaii is the world’s most continuo...            magma   \n",
       "\n",
       "        distractor1         distractor2        correct_answer  \\\n",
       "0          protozoa         gymnosperms  mesophilic organisms   \n",
       "1       muon effect  centrifugal effect       coriolis effect   \n",
       "2        unbalanced            reactive            exothermic   \n",
       "3        beta decay         gamma decay           alpha decay   \n",
       "4  greenhouse gases     carbon and smog         smoke and ash   \n",
       "\n",
       "                                             support  \n",
       "0  Mesophiles grow best in moderate temperature, ...  \n",
       "1  Without Coriolis Effect the global winds would...  \n",
       "2  Summary Changes of state are examples of phase...  \n",
       "3  All radioactive decay is dangerous to living t...  \n",
       "4  Example 3.5 Calculating Projectile Motion: Hot...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>distractor3</th>\n",
       "      <th>distractor1</th>\n",
       "      <th>distractor2</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What type of organism is commonly used in prep...</td>\n",
       "      <td>viruses</td>\n",
       "      <td>protozoa</td>\n",
       "      <td>gymnosperms</td>\n",
       "      <td>mesophilic organisms</td>\n",
       "      <td>Mesophiles grow best in moderate temperature, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What phenomenon makes global winds blow northe...</td>\n",
       "      <td>tropical effect</td>\n",
       "      <td>muon effect</td>\n",
       "      <td>centrifugal effect</td>\n",
       "      <td>coriolis effect</td>\n",
       "      <td>Without Coriolis Effect the global winds would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Changes from a less-ordered state to a more-or...</td>\n",
       "      <td>endothermic</td>\n",
       "      <td>unbalanced</td>\n",
       "      <td>reactive</td>\n",
       "      <td>exothermic</td>\n",
       "      <td>Summary Changes of state are examples of phase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the least dangerous radioactive decay?</td>\n",
       "      <td>zeta decay</td>\n",
       "      <td>beta decay</td>\n",
       "      <td>gamma decay</td>\n",
       "      <td>alpha decay</td>\n",
       "      <td>All radioactive decay is dangerous to living t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kilauea in hawaii is the world’s most continuo...</td>\n",
       "      <td>magma</td>\n",
       "      <td>greenhouse gases</td>\n",
       "      <td>carbon and smog</td>\n",
       "      <td>smoke and ash</td>\n",
       "      <td>Example 3.5 Calculating Projectile Motion: Hot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Format the dataset using the defined prompt formatting function\n",
    "formatted_dataset = dataset.map(format_sciq_prompt)"
   ],
   "id": "488e6d566cd52a98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:47:17.664512Z",
     "start_time": "2025-07-30T09:47:17.659503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Prompt:\\n\" + formatted_dataset[0]['prompt'])\n",
    "print(\"Completion:\\n\" + formatted_dataset[0]['completion'])"
   ],
   "id": "481d06efc29923b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "Given the context: Mesophiles grow best in moderate temperature, typically between 25°C and 40°C (77°F and 104°F). Mesophiles are often found living in or on the bodies of humans or other animals. The optimal growth temperature of many pathogenic mesophiles is 37°C (98°F), the normal human body temperature. Mesophilic organisms have important uses in food preparation, including cheese, yogurt, beer and wine.\n",
      "Generate 1 multiple-choice question(s) with four options each and indicate the correct answer.\n",
      "Completion:\n",
      "Question: What type of organism is commonly used in preparation of foods such as cheese and yogurt??\n",
      "A) protozoa\n",
      "B) gymnosperms\n",
      "C) viruses\n",
      "D) mesophilic organisms\n",
      "Correct Answer: D\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:47:17.727620Z",
     "start_time": "2025-07-30T09:47:17.724243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_example(batch):\n",
    "    texts = [f\"{prompt}\\n{completion}{tokenizer.eos_token}\" for prompt, completion in zip(batch['prompt'], batch['completion'])]\n",
    "    return tokenizer(texts, truncation=True, max_length=512)"
   ],
   "id": "ab35eed37862a504",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:47:17.875055Z",
     "start_time": "2025-07-30T09:47:17.794423Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_dataset = formatted_dataset.map(tokenize_example, batched=True, remove_columns=[\"prompt\", \"completion\"])",
   "id": "a4ece161738e8fa6",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:47:17.946973Z",
     "start_time": "2025-07-30T09:47:17.911909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split dataset for training and evaluation\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)"
   ],
   "id": "9e3cbcdf4d3f3d44",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:47:18.048729Z",
     "start_time": "2025-07-30T09:47:17.987056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import data collator for language modeling tasks\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Define the data collator, disabling masked language modeling (mlm)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ],
   "id": "1798d889e66bfb88",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Model Inference Before Fine-Tuning",
   "id": "6972ac24e09f05dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "inference_prompt_style = (\n",
    "    \"Given the context: {}.\\n\"\n",
    "    \"Generate 1 multiple-choice question with four options and indicate the correct answer.\"\n",
    ")"
   ],
   "id": "9063910595df1d84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:47:18.095713Z",
     "start_time": "2025-07-30T09:47:18.092637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_mcqa(prompt, model, tokenizer, max_length=1000):\n",
    "    inputs = tokenizer(\n",
    "        [prompt.format(prompt)+ tokenizer.eos_token],\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=max_length,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ],
   "id": "ac934e821234e542",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = formatted_dataset[0]['prompt']\n",
    "generated = generate_mcqa(prompt, model, tokenizer)"
   ],
   "id": "8f083bdf21ff9400"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:48:28.561033Z",
     "start_time": "2025-07-30T09:48:05.780922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Inference Before Fine-Tuning:\")\n",
    "print(generated[0])"
   ],
   "id": "353cba9c1c5b22fc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Before Fine-Tuning:\n",
      "Given the context: The water cycle describes how water evaporates from the surface of the earth, rises into the atmosphere, cools and condenses into clouds, and falls back to the surface as precipitation.\n",
      "Generate 1 multiple-choice question with four options and indicate the correct answer. The correct answer must be chosen by the user.\n",
      "The question should be about the process of water evaporating from the surface of the earth.\n",
      "The options should be: A) Water evaporates from the surface of the earth, which is the process of the water cycle. B) Water evaporates from the surface of the earth, which is not the process of the water cycle. C) Water evaporates from the surface of the earth, which is a process of the water cycle. D) Water evaporates from the surface of the earth, which is not a process of the water cycle.\n",
      "To determine if a question is appropriate, you can look at the options and see if they are either both correct or both incorrect. If the options are both correct or both incorrect, then the question is inappropriate.\n",
      "In this case, the options are A, B, C, D. Let me check if the options are both correct or both incorrect.\n",
      "Wait, the question is about the process of water evaporating from the surface of the earth. The options are about whether the process is part of the water cycle or not. So the correct answer should be about the process being a part of the water cycle. So the correct option is C, which says that water evaporates from the surface of the earth, which is a process of the water cycle. The other options are either both correct or both incorrect. Wait, but in the question, the correct answer must be chosen by the user. So the correct answer is C, as it describes the process being a part of the water cycle. The other options are either correct or incorrect. So I need to ensure that the correct answer is about the process being a part of the water cycle, and the other options are either both correct or both incorrect. In this case, options A and D are both incorrect, while options B and C are both correct. So the question is appropriate because the options are both correct or both incorrect.\n",
      "</think>\n",
      "\n",
      "**Question:**  \n",
      "What is true about the process of water evaporating from the surface of the earth?\n",
      "\n",
      "**Options:**  \n",
      "A) Water evaporates from the surface of the earth, which is the process of the water cycle.  \n",
      "B) Water evaporates from the surface of the earth, which is not the process of the water cycle.  \n",
      "C) Water evaporates from the surface of the earth, which is a process of the water cycle.  \n",
      "D) Water evaporates from the surface of the earth, which is not a process of the water cycle.  \n",
      "\n",
      "**Correct Answer:** C) Water evaporates from the surface of the earth, which is a process of the water cycle.  \n",
      "\n",
      "The options are both correct or both incorrect, making the question appropriate.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Setting up the model",
   "id": "63cfe09e4dfc2150"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import LoRA configuration and model wrapping utilities\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,  # Scaling factor for LoRA\n",
    "    lora_dropout=0.05,  # Add a slight dropout for regularization\n",
    "    r=64,  # Rank of the LoRA update matrices\n",
    "    bias=\"none\",  # No bias reparameterization\n",
    "    task_type=\"CAUSAL_LM\",  # Task type: Causal Language Modeling\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # Target modules for LoRA\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA configuration\n",
    "model = get_peft_model(model, peft_config)"
   ],
   "id": "3bb1acc53a92c778",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import the SFTTrainer for supervised fine-tuning\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training Arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"../models/DeepSeek-R1-Distill-Qwen-1.5B-SciQ\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    # fp16=not is_bfloat16_supported(), # Use FP16 if BF16 is not supported\n",
    "    # bf16=is_bfloat16_supported(),     # Use BF16 if supported\n",
    "    group_by_length=True,\n",
    "    report_to=\"none\",  # Disable logging to external tools for simplicity\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=formatted_dataset,\n",
    "    peft_config=peft_config,\n",
    "    data_collator=data_collator,\n",
    ")"
   ],
   "id": "9216c2cded2c655b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Model Training",
   "id": "1c081675cdd8c283"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Clear GPU memory and disable cache before training\n",
    "import gc, torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Start the training process\n",
    "trainer.train()"
   ],
   "id": "15c10dba217dafac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Access log history after training\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Convert to DataFrame for easier viewing and saving\n",
    "df_logs = pd.DataFrame(log_history)\n",
    "\n",
    "# Print the last few log entries\n",
    "print(df_logs.tail())\n",
    "\n",
    "# Optionally, save to CSV\n",
    "df_logs.to_csv(\"training_metrics.csv\", index=False)\n",
    "print(\"Training metrics saved to training_metrics.csv\")"
   ],
   "id": "a8b8e83a04eb136"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Model inference after fine-tuning",
   "id": "c464db27e65e941d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate the model inferences after fine-tuning\n",
    "prompt = formatted_dataset[0]['prompt']\n",
    "generated = generate_mcqa(prompt, model, tokenizer)\n",
    "print(\"Inference Before Fine-Tuning:\")\n",
    "print(generated[0])"
   ],
   "id": "e60ab098601c484d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Saving the model",
   "id": "ba7a6dd476bbe8db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Push the fine-tuned model and tokenizer to Hugging Face Hub\n",
    "new_model_name = \"DeepSeek-R1-Distill-Qwen-1.5B-SciQ\"\n",
    "trainer.model.push_to_hub(new_model_name)\n",
    "trainer.processing_class.push_to_hub(new_model_name)"
   ],
   "id": "69a7f3064e5f19fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. Loading the Adopter and testing the model",
   "id": "9a8062a390039578"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Clean up model and trainer objects, and clear GPU memory\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "94e3cf67af457f1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from huggingface_hub import whoami\n",
    "\n",
    "# Get the current user's information\n",
    "user_info = whoami()\n",
    "\n",
    "# Extract the username\n",
    "hf_user = user_info['name']\n",
    "print(f\"Username: {hf_user}\")"
   ],
   "id": "141d26f917a148f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Re-import necessary libraries for loading the fine-tuned model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Base model\n",
    "base_model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "# Your fine-tuned LoRA adapter repository\n",
    "lora_adapter_id = f\"{hf_user}/DeepSeek-R1-Distill-Qwen-1.5B-SciQ\"\n",
    "\n",
    "# Load the model in 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Attach the LoRA adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    lora_adapter_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)"
   ],
   "id": "183a8903840855a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the medical reasoning dataset\n",
    "testing_set = load_dataset(\n",
    "    \"allenai/sciq\",\n",
    "    split=\"test\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ],
   "id": "2300bd94111f9375"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for record in testing_set:\n",
    "    # Format the prompt for inference\n",
    "    prompt = format_sciq_prompt(record, num_questions=1, include_cot=False)['prompt']\n",
    "\n",
    "    # Generate a response using the model\n",
    "    generated = generate_mcqa(prompt, model, tokenizer)\n",
    "\n",
    "    # Add generated response to the record\n",
    "    record['generated_response'] = generated[0]"
   ],
   "id": "f56afcb0086f4fe5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Show the first 5 records with generated responses\n",
    "df = pd.DataFrame(testing_set)\n",
    "df.head()"
   ],
   "id": "64101f37ca95a4e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the testing set with generated responses to a CSV file\n",
    "output_file = \"sciq_test_with_responses.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Testing set with generated responses saved to {output_file}\")"
   ],
   "id": "fb35c77dbe6b373f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
